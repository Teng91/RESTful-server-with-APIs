{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel is ready.\n"
     ]
    }
   ],
   "source": [
    "# Check kernel\n",
    "print(\"Kernel is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'\n",
    "\n",
    "# 列出資料夾中的所有檔案\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 用來儲存所有資料框 (DataFrame) 的列表\n",
    "data_frames = []\n",
    "\n",
    "# 逐一讀取每個 CSV 檔案\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "# 合併所有資料框\n",
    "all_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# 顯示合併後的資料\n",
    "print(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'\n",
    "\n",
    "# 列出資料夾中的所有檔案\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 用來儲存每個檔案的最大 count 值\n",
    "max_counts = []\n",
    "\n",
    "# 逐一讀取每個 CSV 檔案\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 確保資料中包含 'count' 欄位\n",
    "    if 'Count' in df.columns:\n",
    "        max_count = df['Count'].max()  # 找出 'count' 欄位的最大值\n",
    "        max_counts.append((file_name, max_count))  # 儲存檔案名稱與對應的最大值\n",
    "    else:\n",
    "        print(f\"檔案 {file_name} 中找不到 'Count' 欄位。\")\n",
    "\n",
    "# 顯示每個檔案的最大 count 值\n",
    "for file_name, max_count in max_counts:\n",
    "    print(f\"檔案 {file_name} 的最大 Count 值是: {max_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'\n",
    "\n",
    "# 列出資料夾中的所有檔案\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 用來儲存每個檔案的最大 count 值及其檔案名稱\n",
    "max_counts = []\n",
    "file_names_for_max_count = []\n",
    "\n",
    "# 逐一讀取每個 CSV 檔案\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 確保資料中包含 'Count' 欄位\n",
    "    if 'Count' in df.columns:\n",
    "        max_count = df['Count'].max()  # 找出 'Count' 欄位的最大值\n",
    "        max_counts.append(max_count)  # 儲存最大值\n",
    "        file_names_for_max_count.append(file_name)  # 儲存對應檔案名稱\n",
    "    else:\n",
    "        print(f\"檔案 {file_name} 中找不到 'Count' 欄位。\")\n",
    "\n",
    "# 找出最大值和最小值\n",
    "if max_counts:\n",
    "    overall_max = max(max_counts)  # 最大值\n",
    "    overall_min = min(max_counts)  # 最小值\n",
    "\n",
    "    # 找出最大值所在的檔案\n",
    "    max_index = max_counts.index(overall_max)\n",
    "    file_with_max_count = file_names_for_max_count[max_index]\n",
    "\n",
    "    # 找出最小值所在的檔案\n",
    "    min_index = max_counts.index(overall_min)\n",
    "    file_with_min_count = file_names_for_max_count[min_index]\n",
    "\n",
    "    # 顯示結果\n",
    "    print(f\"所有檔案中的最大 Count 值是: {overall_max}，來自檔案: {file_with_max_count}\")\n",
    "    print(f\"所有檔案中的最小 Count 值是: {overall_min}，來自檔案: {file_with_min_count}\")\n",
    "else:\n",
    "    print(\"沒有找到任何有效的 'Count' 欄位。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'\n",
    "\n",
    "# 列出資料夾中的所有檔案\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 用來儲存每個檔案的最大 count 值及其檔案名稱\n",
    "max_counts = []\n",
    "file_names_for_max_count = []\n",
    "\n",
    "# 逐一讀取每個 CSV 檔案\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 確保資料中包含 'Count' 欄位\n",
    "    if 'Count' in df.columns:\n",
    "        max_count = df['Count'].max()  # 找出 'Count' 欄位的最大值\n",
    "        max_counts.append(max_count)  # 儲存最大值\n",
    "        file_names_for_max_count.append(file_name)  # 儲存對應檔案名稱\n",
    "    else:\n",
    "        print(f\"檔案 {file_name} 中找不到 'Count' 欄位。\")\n",
    "\n",
    "# 繪製折線圖\n",
    "if max_counts:\n",
    "    # 設定圖表\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(max_counts) + 1), max_counts, linestyle='-', color='b')\n",
    "\n",
    "    # 設定標題與軸標籤\n",
    "    plt.title('Max_count for each Files_index')\n",
    "    plt.xlabel('Files_index')\n",
    "    plt.ylabel('Max_count')\n",
    "\n",
    "    # 顯示圖表\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"沒有找到任何有效的 'Count' 欄位。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def calculate_laplacian_variance(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"無法讀取影像: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 計算 Laplacian，並計算 Laplacian 的方差\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    # 取得資料夾內所有影像檔案\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    # 計算每張影像的 Laplacian Variance\n",
    "    variances = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        variance = calculate_laplacian_variance(image_path)\n",
    "        if variance is not None:\n",
    "            variances.append([image_file, variance])\n",
    "    \n",
    "    return variances\n",
    "\n",
    "def write_variances_to_csv(variances, output_csv):\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Name\", \"Laplacian Variance\"])  # 寫入標題行\n",
    "        writer.writerows(variances)  # 寫入數據\n",
    "\n",
    "# 需要處理的資料夾路徑\n",
    "# folder_path = 'D:/test/SAG_green_demo_board/B'  # 替換成你的資料夾路徑\n",
    "folder_path = 'D:/tsne_analysis/image/B'\n",
    "\n",
    "# 計算並取得 Laplacian Variance\n",
    "variances = process_images_in_folder(folder_path)\n",
    "\n",
    "# 將結果寫入 CSV 檔案\n",
    "output_csv = 'Laplacian_Variance.csv'\n",
    "write_variances_to_csv(variances, output_csv)\n",
    "\n",
    "print(f\"結果已儲存至 {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means(Laplacian Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_laplacian_variance(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"無法讀取影像: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 計算 Laplacian，並計算 Laplacian 的方差\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    # 取得資料夾內所有影像檔案\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    # 計算每張影像的 Laplacian Variance\n",
    "    variances = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        variance = calculate_laplacian_variance(image_path)\n",
    "        if variance is not None:\n",
    "            variances.append([image_file, variance])\n",
    "    \n",
    "    return variances\n",
    "\n",
    "def apply_kmeans_to_variances(variances, num_clusters=3):\n",
    "    # 提取 Laplacian Variance 的數據\n",
    "    data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "\n",
    "    # 使用 K-means 算法將 Laplacian Variance 分為 2 類\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # 把每個影像和其對應的類別標註\n",
    "    for i in range(len(variances)):\n",
    "        variances[i].append(labels[i])\n",
    "    \n",
    "    return variances, kmeans\n",
    "\n",
    "def write_variances_to_csv(variances, output_csv):\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Name\", \"Laplacian Variance\", \"Cluster Label\"])  # 寫入標題行\n",
    "        writer.writerows(variances)  # 寫入數據\n",
    "\n",
    "# 需要處理的資料夾路徑\n",
    "folder_path = 'D:/tsne_analysis/image/B'\n",
    "\n",
    "# 計算並取得 Laplacian Variance\n",
    "variances = process_images_in_folder(folder_path)\n",
    "\n",
    "# 使用 K-means 分類 Laplacian Variance\n",
    "variances, kmeans = apply_kmeans_to_variances(variances)\n",
    "\n",
    "# 將結果寫入 CSV 檔案\n",
    "# output_csv = 'Laplacian_Variance_with_clusters.csv'\n",
    "# write_variances_to_csv(variances, output_csv)\n",
    "# print(f\"結果已儲存至 {output_csv}\")\n",
    "\n",
    "# 可視化 K-means 結果（選擇性）\n",
    "data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "plt.scatter(data, np.zeros_like(data), c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.title(\"K-means Clustering of Laplacian Variance\")\n",
    "plt.xlabel(\"Laplacian Variance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means(Laplacian Variance & Image Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_laplacian_variance(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"無法讀取影像: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 計算 Laplacian，並計算 Laplacian 的方差\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    # 取得資料夾內所有影像檔案\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    # 計算每張影像的 Laplacian Variance 和影像大小（寬度 * 高度）\n",
    "    features = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        \n",
    "        # 計算 Laplacian Variance\n",
    "        variance = calculate_laplacian_variance(image_path)\n",
    "        if variance is None:\n",
    "            continue\n",
    "        \n",
    "        # 取得影像尺寸 (寬度 * 高度)\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width = image.shape[:2]\n",
    "        size = width * height  # 使用圖像的寬高積作為大小指標\n",
    "        \n",
    "        features.append([image_file, variance, size])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def apply_kmeans_to_features(features, num_clusters=3):\n",
    "    # 提取 Laplacian Variance 和 Image Size 作為特徵\n",
    "    data = np.array([[f[1], f[2]] for f in features])  # 以 Laplacian Variance 和 Image Size 作為特徵\n",
    "\n",
    "    # 使用 K-means 算法將特徵分為多個類別\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # 把每個影像和其對應的類別標註\n",
    "    for i in range(len(features)):\n",
    "        features[i].append(labels[i])\n",
    "    \n",
    "    return features, kmeans, data\n",
    "\n",
    "def write_features_to_csv(features, output_csv):\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Name\", \"Laplacian Variance\", \"Image Size\", \"Cluster Label\"])  # 寫入標題行\n",
    "        writer.writerows(features)  # 寫入數據\n",
    "\n",
    "# 需要處理的資料夾路徑\n",
    "folder_path = 'D:/tsne_analysis/image/B'\n",
    "\n",
    "# 計算並取得影像特徵\n",
    "features = process_images_in_folder(folder_path)\n",
    "\n",
    "# 使用 K-means 分類 Laplacian Variance 和 Image Size\n",
    "features, kmeans, data = apply_kmeans_to_features(features)\n",
    "\n",
    "# 將結果寫入 CSV 檔案\n",
    "# output_csv = 'Laplacian_Variance_and_Size_with_clusters.csv'\n",
    "# write_features_to_csv(features, output_csv)\n",
    "# print(f\"結果已儲存至 {output_csv}\")\n",
    "\n",
    "# 可視化 K-means 結果（Laplacian Variance 和 Image Size 在 2D 空間中）\n",
    "plt.scatter(data[:, 0], data[:, 1], c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.title(\"K-means Clustering of Laplacian Variance and Image Size\")\n",
    "plt.xlabel(\"Laplacian Variance\")\n",
    "plt.ylabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12978 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_laplacian_variance(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"無法讀取影像: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 計算 Laplacian，並計算 Laplacian 的方差\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    # 取得資料夾內所有影像檔案\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    # 計算每張影像的 Laplacian Variance\n",
    "    variances = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        variance = calculate_laplacian_variance(image_path)\n",
    "        if variance is not None:\n",
    "            variances.append([image_file, variance])\n",
    "    \n",
    "    return variances\n",
    "\n",
    "def apply_kmeans_to_variances(variances, num_clusters=5):\n",
    "    # 提取 Laplacian Variance 的數據\n",
    "    data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "\n",
    "    # 使用 K-means 算法將 Laplacian Variance 分為 2 類\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # 把每個影像和其對應的類別標註\n",
    "    for i in range(len(variances)):\n",
    "        variances[i].append(labels[i])\n",
    "    \n",
    "    return variances, kmeans\n",
    "\n",
    "def calculate_cluster_boundaries(variances, kmeans):\n",
    "    # 根據 K-means 聚類的標籤來計算每個聚類的 Laplacian Variance 邊界（最小值和最大值）\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # 創建一個字典來存儲每個聚類的 Laplacian Variance 範圍\n",
    "    cluster_boundaries = {}\n",
    "\n",
    "    for i in range(len(variances)):\n",
    "        image_file, variance, cluster_label = variances[i]\n",
    "        if cluster_label not in cluster_boundaries:\n",
    "            cluster_boundaries[cluster_label] = {'min': variance, 'max': variance}\n",
    "        else:\n",
    "            cluster_boundaries[cluster_label]['min'] = min(cluster_boundaries[cluster_label]['min'], variance)\n",
    "            cluster_boundaries[cluster_label]['max'] = max(cluster_boundaries[cluster_label]['max'], variance)\n",
    "\n",
    "    # 輸出每個聚類的邊界\n",
    "    for cluster_label, boundary in cluster_boundaries.items():\n",
    "        print(f\"Cluster {cluster_label}: Min Laplacian Variance = {boundary['min']}, Max Laplacian Variance = {boundary['max']}\")\n",
    "\n",
    "    return cluster_boundaries\n",
    "\n",
    "# 假設 variances 已經包含了所有影像的 Laplacian Variance 和其聚類標籤\n",
    "cluster_boundaries = calculate_cluster_boundaries(variances, kmeans)\n",
    "\n",
    "def write_variances_to_csv(variances, output_csv):\n",
    "    # 寫入 CSV 檔案\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Image Name\", \"Laplacian Variance\", \"Cluster Label\"])  # 寫入標題行\n",
    "        writer.writerows(variances)  # 寫入數據\n",
    "\n",
    "# 需要處理的資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B'\n",
    "\n",
    "# 計算並取得 Laplacian Variance\n",
    "variances = process_images_in_folder(folder_path)\n",
    "\n",
    "# 使用 K-means 分類 Laplacian Variance\n",
    "variances, kmeans = apply_kmeans_to_variances(variances)\n",
    "\n",
    "# 將結果寫入 CSV 檔案\n",
    "output_csv = 'Laplacian_Variance_with_clusters_12978.csv'\n",
    "write_variances_to_csv(variances, output_csv)\n",
    "print(f\"結果已儲存至 {output_csv}\")\n",
    "\n",
    "# 可視化 K-means 結果（選擇性）\n",
    "data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "plt.scatter(data, np.zeros_like(data), c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.title(\"K-means Clustering of Laplacian Variance\")\n",
    "plt.xlabel(\"Laplacian Variance\")\n",
    "plt.show()\n",
    "\n",
    "# 顯示每個聚類的中心（即每個聚類的 Laplacian Variance 平均值）\n",
    "print(\"K-means Cluster Centers (Laplacian Variance):\")\n",
    "print(kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means: Square(10534), Circle(1684), Rect(760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_laplacian_variance(image_path):\n",
    "    # 讀取影像\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"無法讀取影像: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 計算 Laplacian，並計算 Laplacian 的方差\n",
    "    laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    # 取得資料夾內所有影像檔案\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "    \n",
    "    # 計算每張影像的 Laplacian Variance\n",
    "    variances = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        variance = calculate_laplacian_variance(image_path)\n",
    "        if variance is not None:\n",
    "            variances.append([image_file, variance])\n",
    "    \n",
    "    return variances\n",
    "\n",
    "def apply_kmeans_to_variances(variances, num_clusters=5):\n",
    "    # 提取 Laplacian Variance 的數據\n",
    "    data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "\n",
    "    # 使用 K-means 算法將 Laplacian Variance 分為 2 類\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "\n",
    "    # 把每個影像和其對應的類別標註\n",
    "    for i in range(len(variances)):\n",
    "        variances[i].append(labels[i])\n",
    "    \n",
    "    return variances, kmeans\n",
    "\n",
    "def calculate_cluster_boundaries(variances, kmeans):\n",
    "    # 根據 K-means 聚類的標籤來計算每個聚類的 Laplacian Variance 邊界（最小值和最大值）\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # 創建一個字典來存儲每個聚類的 Laplacian Variance 範圍\n",
    "    cluster_boundaries = {}\n",
    "\n",
    "    for i in range(len(variances)):\n",
    "        image_file, variance, cluster_label = variances[i]\n",
    "        if cluster_label not in cluster_boundaries:\n",
    "            cluster_boundaries[cluster_label] = {'min': variance, 'max': variance}\n",
    "        else:\n",
    "            cluster_boundaries[cluster_label]['min'] = min(cluster_boundaries[cluster_label]['min'], variance)\n",
    "            cluster_boundaries[cluster_label]['max'] = max(cluster_boundaries[cluster_label]['max'], variance)\n",
    "\n",
    "    # 輸出每個聚類的邊界，並將 Laplacian Variance 保留到小數點後兩位\n",
    "    for cluster_label, boundary in cluster_boundaries.items():\n",
    "        print(f\"Cluster {cluster_label}: Min Laplacian Variance = {round(boundary['min'], 2)}, Max Laplacian Variance = {round(boundary['max'], 2)}\")\n",
    "\n",
    "    return cluster_boundaries\n",
    "\n",
    "# 需要處理的資料夾路徑\n",
    "folder_path = 'D:/test/SAG_green_demo_board/KMeans/B_circle'\n",
    "\n",
    "# 計算並取得 Laplacian Variance\n",
    "variances = process_images_in_folder(folder_path)\n",
    "\n",
    "# 使用 K-means 分類 Laplacian Variance\n",
    "variances, kmeans = apply_kmeans_to_variances(variances)\n",
    "\n",
    "# 計算聚類的邊界\n",
    "cluster_boundaries = calculate_cluster_boundaries(variances, kmeans)\n",
    "\n",
    "# 可視化 K-means 結果（選擇性）\n",
    "data = np.array([v[1] for v in variances]).reshape(-1, 1)\n",
    "plt.scatter(data, np.zeros_like(data), c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.title(\"K-means Clustering of Laplacian Variance\")\n",
    "plt.xlabel(\"Laplacian Variance\")\n",
    "plt.show()\n",
    "\n",
    "# 顯示每個聚類的中心（即每個聚類的 Laplacian Variance 平均值）\n",
    "print(\"K-means Cluster Centers (Laplacian Variance):\")\n",
    "print(np.round(kmeans.cluster_centers_, 2))\n",
    "\n",
    "# 如果需要，可以將結果寫入 CSV 文件\n",
    "output_csv = 'Laplacian_Variance_with_clusters_circle.csv'\n",
    "write_variances_to_csv(variances, output_csv)\n",
    "print(f\"結果已儲存至 {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Valley Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讀取CSV檔\n",
    "df = pd.read_csv('D:/test/SAG_green_demo_board/B_csv_analysis/UT4_1_107_B.csv')\n",
    "\n",
    "# 假設CSV檔中有兩欄資料，一欄是灰階值，另一欄是對應的數量\n",
    "intensities = df['Intensity'].values  # 灰階值\n",
    "counts = df['Count'].values      # 數量\n",
    "\n",
    "# 將數據轉換為numpy陣列\n",
    "intensities = np.array(intensities)\n",
    "counts = np.array(counts)\n",
    "\n",
    "# 在指定範圍內尋找波峰\n",
    "def find_peak_in_range(intensities, counts, start, end):\n",
    "    mask = (intensities >= start) & (intensities <= end)\n",
    "    peak_index = np.argmax(counts[mask])\n",
    "    peak_intensity = intensities[mask][peak_index]\n",
    "    peak_count = counts[mask][peak_index]\n",
    "    return peak_intensity, peak_count\n",
    "\n",
    "# 在兩個範圍內尋找波峰\n",
    "peak1_intensity, peak1_count = find_peak_in_range(intensities, counts, 0, 50)\n",
    "peak2_intensity, peak2_count = find_peak_in_range(intensities, counts, 60, 110)\n",
    "\n",
    "# 繪製圖表\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(intensities, counts, 'b-', label='Intensity Distribution')\n",
    "plt.plot(peak1_intensity, peak1_count, 'ro', label=f'Peak 1 ({peak1_intensity}, {peak1_count})')\n",
    "plt.plot(peak2_intensity, peak2_count, 'go', label=f'Peak 2 ({peak2_intensity}, {peak2_count})')\n",
    "plt.xlabel('Intensity')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Intensity Distribution with Peaks')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"First peak (0-50): Intensity = {peak1_intensity}, Count = {peak1_count}\")\n",
    "print(f\"Second peak (60-110): Intensity = {peak2_intensity}, Count = {peak2_count}\")\n",
    "\n",
    "# 顯示圖表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLR(手動設定線段數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义将数据分成8个区间并进行线性回归的函数\n",
    "def piecewise_linear_regression(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    segments = []\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        segments.append((x_segment, model.predict(x_segment)))\n",
    "    \n",
    "    return breakpoints, segments\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_folder = 'D:/test/SAG_green_demo_board/PLR_8/'  # 保存结果的文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历文件夹中的所有 CSV 文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # 读取 CSV 文件\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # 假设 CSV 文件中有 'Intensity' 和 'Count' 两列\n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保数据长度一致\n",
    "        assert len(intensities) == len(counts), \"数据列长度不一致\"\n",
    "        \n",
    "        # 转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 执行线性回归分析\n",
    "        num_segments = 8  # 设置分段数\n",
    "        breakpoints, segments = piecewise_linear_regression(x, y, num_segments)\n",
    "        \n",
    "        # 合并分段的拟合结果\n",
    "        x_fit = np.concatenate([segment[0] for segment in segments])\n",
    "        y_fit = np.concatenate([segment[1] for segment in segments])\n",
    "        \n",
    "        # 绘制图表\n",
    "        plt.plot(x, y, label='Original Data', color='b')  # 原始数据\n",
    "        plt.plot(x_fit, y_fit, label='Piecewise Linear Regression', color='r')  # 回归线\n",
    "        for bp in breakpoints:\n",
    "            plt.axvline(x=x[bp], color='g', linestyle='--')  # 断点标记\n",
    "\n",
    "        # 设置图表标签和标题\n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Piecewise Linear Regression: {filename}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 保存图表\n",
    "        output_path = os.path.join(output_folder, f'{os.path.splitext(filename)[0]}_regression.png')\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()  # 关闭当前图表，准备下一个文件\n",
    "\n",
    "print(\"所有图表已保存。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLR(8線段)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义将数据分成8个区间并进行线性回归的函数\n",
    "def piecewise_linear_regression(x, y, num_segments):\n",
    "    # 计算断点\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    segments = []\n",
    "    piecewise_endpoints = []  # 用于保存每个分段的端点\n",
    "\n",
    "    # 对每个区间进行线性回归\n",
    "    model = LinearRegression()\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 记录每段的回归线\n",
    "        segments.append((x_segment, model.predict(x_segment)))\n",
    "        \n",
    "        # 保存该段的端点坐标（起始点和结束点）\n",
    "        piecewise_endpoints.append(((x_segment[0][0], y_segment[0]), (x_segment[-1][0], y_segment[-1])))\n",
    "\n",
    "    return breakpoints, segments, piecewise_endpoints\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_folder = 'D:/test/SAG_green_demo_board/PLR_8_single/'  # 保存结果的文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历文件夹中的所有 CSV 文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # 读取 CSV 文件\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # 假设 CSV 文件中有 'Intensity' 和 'Count' 两列\n",
    "        intensities = df['Intensity'].values  # 灰阶值\n",
    "        counts = df['Count'].values      # 数量\n",
    "\n",
    "        # 确保 'Intensity' 和 'Count' 列的长度一致\n",
    "        assert len(intensities) == len(counts), \"数据列长度不一致\"\n",
    "\n",
    "        # 将数据转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)  # 需要将 x 转换为二维数组\n",
    "        y = counts\n",
    "\n",
    "        # 使用Piecewise Linear Regression方法得到断点和回归线\n",
    "        num_segments = 8  # 设定我们要的段数\n",
    "        breakpoints, segments, piecewise_endpoints = piecewise_linear_regression(x, y, num_segments)\n",
    "\n",
    "        # 绘制结果\n",
    "        plt.plot(x, y, label='Original Data', color='b')  # 原始数据\n",
    "\n",
    "        # 单独绘制每个回归段的线并标记图例\n",
    "        for i, segment in enumerate(segments, start=1):\n",
    "            plt.plot(segment[0], segment[1], label=f'Segment {i}', color='r')\n",
    "\n",
    "        # 设置图表标签和标题\n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Piecewise Linear Regression: {filename}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 保存图表\n",
    "        output_path = os.path.join(output_folder, f'{os.path.splitext(filename)[0]}_regression.png')\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()  # 关闭当前图表，准备下一个文件\n",
    "\n",
    "print(\"所有图表已保存。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLR(端點連接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义将数据分成8个区间并进行线性回归的函数\n",
    "def piecewise_linear_regression(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    segments = []\n",
    "    piecewise_endpoints = []  # 用于保存每个分段的端点\n",
    "\n",
    "    model = LinearRegression()\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 记录每段的回归线\n",
    "        segments.append((x_segment, model.predict(x_segment)))\n",
    "        \n",
    "        # 保存该段的端点坐标（起始点和结束点）\n",
    "        piecewise_endpoints.append(((x_segment[0][0], y_segment[0]), (x_segment[-1][0], y_segment[-1])))\n",
    "\n",
    "    return breakpoints, segments, piecewise_endpoints\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_folder = 'D:/test/SAG_green_demo_board/PLR_endpoints/'  # 保存结果的文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历文件夹中的所有 CSV 文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # 读取 CSV 文件\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # 假设 CSV 文件中有 'Intensity' 和 'Count' 两列\n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保数据长度一致\n",
    "        assert len(intensities) == len(counts), \"数据列长度不一致\"\n",
    "        \n",
    "        # 转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 执行线性回归分析\n",
    "        num_segments = 8  # 设置分段数\n",
    "        breakpoints, segments, piecewise_endpoints = piecewise_linear_regression(x, y, num_segments)\n",
    "        \n",
    "        # 提取端点的坐标\n",
    "        endpoints_x = []\n",
    "        endpoints_y = []\n",
    "        for start_end in piecewise_endpoints:\n",
    "            endpoints_x.extend([start_end[0][0], start_end[1][0]])  # 获取端点的x值\n",
    "            endpoints_y.extend([start_end[0][1], start_end[1][1]])  # 获取端点的y值\n",
    "        \n",
    "        # 绘制结果：直接连接端点\n",
    "        plt.plot(x, y, label='Original Data', color='b')  # 原始数据\n",
    "        plt.plot(endpoints_x, endpoints_y, label='Piecewise Endpoints', color='r', marker='o')  # 连接端点的线\n",
    "        \n",
    "        # 设置图表标签和标题\n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Connecting Endpoints of Piecewise Linear Regression: {filename}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 保存图表\n",
    "        output_path = os.path.join(output_folder, f'{os.path.splitext(filename)[0]}_endpoints.png')\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()  # 关闭当前图表，准备下一个文件\n",
    "\n",
    "print(\"所有图表已保存。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLR(AIC/BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义函数计算AIC和BIC\n",
    "def calculate_aic_bic(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # 样本大小\n",
    "    k = 2  # 每个回归段的参数个数（线性回归有2个参数：斜率和截距）\n",
    "    \n",
    "    residual_sum_of_squares = 0\n",
    "\n",
    "    # 对每个区间进行线性回归并计算残差平方和\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 计算该段的预测值\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)  # 残差平方和\n",
    "\n",
    "    # 计算 AIC 和 BIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    bic = n * np.log(residual_sum_of_squares / n) + k * num_segments * np.log(n)\n",
    "    \n",
    "    return aic, bic\n",
    "\n",
    "# 读取数据并执行Piecewise回归的函数\n",
    "def piecewise_linear_regression(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    segments = []\n",
    "    piecewise_endpoints = []  # 用于保存每个分段的端点\n",
    "\n",
    "    # 对每个区间进行线性回归\n",
    "    model = LinearRegression()\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 记录每段的回归线\n",
    "        segments.append((x_segment, model.predict(x_segment)))\n",
    "        \n",
    "        # 保存该段的端点坐标（起始点和结束点）\n",
    "        piecewise_endpoints.append(((x_segment[0][0], y_segment[0]), (x_segment[-1][0], y_segment[-1])))\n",
    "\n",
    "    return breakpoints, segments, piecewise_endpoints\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_aic_folder = 'D:/test/SAG_green_demo_board/PLR_AIC/'  # 保存结果的文件夹路径\n",
    "output_bic_folder = 'D:/test/SAG_green_demo_board/PLR_BIC/'  # 保存结果的文件夹路径\n",
    "\n",
    "if not os.path.exists(output_aic_folder):\n",
    "    os.makedirs(output_aic_folder)\n",
    "    \n",
    "if not os.path.exists(output_bic_folder):\n",
    "    os.makedirs(output_bic_folder)\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保 'Intensity' 和 'Count' 列的长度一致\n",
    "        assert len(intensities) == len(counts), f\"数据列长度不一致: {filename}\"\n",
    "        \n",
    "        # 将数据转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 使用AIC和BIC选择最佳分段数\n",
    "        segment_range = range(2, 21)\n",
    "        aic_values = []\n",
    "        bic_values = []\n",
    "        \n",
    "        for num_segments in segment_range:\n",
    "            aic, bic = calculate_aic_bic(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "            bic_values.append(bic)\n",
    "\n",
    "        # 找到具有最小AIC和BIC的分段数\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "        best_num_segments_bic = segment_range[np.argmin(bic_values)]\n",
    "\n",
    "        # 基于AIC和BIC分别进行Piecewise回归并绘图\n",
    "        breakpoints_aic, segments_aic, piecewise_endpoints_aic = piecewise_linear_regression(x, y, best_num_segments_aic)\n",
    "        breakpoints_bic, segments_bic, piecewise_endpoints_bic = piecewise_linear_regression(x, y, best_num_segments_bic)\n",
    "\n",
    "        # 提取AIC的端点坐标\n",
    "        endpoints_x_aic = []\n",
    "        endpoints_y_aic = []\n",
    "        for start_end in piecewise_endpoints_aic:\n",
    "            endpoints_x_aic.extend([start_end[0][0], start_end[1][0]])  \n",
    "            endpoints_y_aic.extend([start_end[0][1], start_end[1][1]])  \n",
    "\n",
    "        # 提取BIC的端点坐标\n",
    "        endpoints_x_bic = []\n",
    "        endpoints_y_bic = []\n",
    "        for start_end in piecewise_endpoints_bic:\n",
    "            endpoints_x_bic.extend([start_end[0][0], start_end[1][0]])  \n",
    "            endpoints_y_bic.extend([start_end[0][1], start_end[1][1]])  \n",
    "\n",
    "        # 绘制AIC的Piecewise回归结果\n",
    "        plt.plot(x, y, label='Original Data', color='b')  \n",
    "        plt.plot(endpoints_x_aic, endpoints_y_aic, label=f'Piecewise (AIC, {best_num_segments_aic} segments)', color='r', marker='o')  \n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Piecewise Linear Regression based on AIC ({best_num_segments_aic} Segments)')\n",
    "        plt.legend()\n",
    "\n",
    "        # 保存到AIC文件夹\n",
    "        plt.savefig(os.path.join(output_aic_folder, f\"PLR_AIC_{filename}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # 绘制BIC的Piecewise回归结果\n",
    "        plt.plot(x, y, label='Original Data', color='b')  \n",
    "        plt.plot(endpoints_x_bic, endpoints_y_bic, label=f'Piecewise (BIC, {best_num_segments_bic} segments)', color='g', marker='o')  \n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Piecewise Linear Regression based on BIC ({best_num_segments_bic} Segments)')\n",
    "        plt.legend()\n",
    "\n",
    "        # 保存到BIC文件夹\n",
    "        plt.savefig(os.path.join(output_bic_folder, f\"PLR_BIC_{filename}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記錄最小分段數(AIC/BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义函数计算AIC和BIC\n",
    "def calculate_aic_bic(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # 样本大小\n",
    "    k = 2  # 每个回归段的参数个数（线性回归有2个参数：斜率和截距）  \n",
    "    residual_sum_of_squares = 0\n",
    "\n",
    "    # 对每个区间进行线性回归并计算残差平方和\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 计算该段的预测值\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)  # 残差平方和\n",
    "\n",
    "    # 计算 AIC 和 BIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    bic = n * np.log(residual_sum_of_squares / n) + k * num_segments * np.log(n)\n",
    "    \n",
    "    return aic, bic\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_csv_folder = 'D:/test/SAG_green_demo_board/'  # 保存结果的文件夹路径\n",
    "\n",
    "# 初始化字典来存储AIC和BIC的最小分段数\n",
    "aic_results = {}\n",
    "bic_results = {}\n",
    "\n",
    "if not os.path.exists(output_csv_folder):\n",
    "    os.makedirs(output_csv_folder)\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保 'Intensity' 和 'Count' 列的长度一致\n",
    "        assert len(intensities) == len(counts), f\"数据列长度不一致: {filename}\"\n",
    "        \n",
    "        # 将数据转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 使用AIC和BIC选择最佳分段数\n",
    "        segment_range = range(2, 21)\n",
    "        aic_values = []\n",
    "        bic_values = []\n",
    "        \n",
    "        for num_segments in segment_range:\n",
    "            aic, bic = calculate_aic_bic(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "            bic_values.append(bic)\n",
    "\n",
    "        # 找到具有最小AIC和BIC的分段数\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "        best_num_segments_bic = segment_range[np.argmin(bic_values)]\n",
    "\n",
    "        # 保存AIC和BIC的最小分段数\n",
    "        aic_results[filename] = best_num_segments_aic\n",
    "        bic_results[filename] = best_num_segments_bic\n",
    "\n",
    "# 将AIC和BIC的最小分段数保存到CSV文件\n",
    "aic_df = pd.DataFrame(list(aic_results.items()), columns=['Filename', 'Best_Num_Segments_AIC'])\n",
    "bic_df = pd.DataFrame(list(bic_results.items()), columns=['Filename', 'Best_Num_Segments_BIC'])\n",
    "\n",
    "aic_df.to_csv(os.path.join(output_csv_folder, 'AIC.csv'), index=False)\n",
    "bic_df.to_csv(os.path.join(output_csv_folder, 'BIC.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算PLR分段斜率(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define function to calculate AIC and slopes of each segment\n",
    "def calculate_aic(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # Sample size\n",
    "    k = 2  # Number of parameters per segment (slope and intercept)\n",
    "    residual_sum_of_squares = 0\n",
    "    segment_slopes = []  # To store the slopes of each segment\n",
    "\n",
    "    # Perform linear regression on each segment and calculate residual sum of squares\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "\n",
    "        # Calculate the predicted values for the segment\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)\n",
    "\n",
    "        # Store the slope of the segment (model.coef_ contains the slope)\n",
    "        segment_slopes.append(model.coef_[0])\n",
    "\n",
    "    # Calculate AIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    \n",
    "    return aic, segment_slopes\n",
    "\n",
    "# Folder paths\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # Change to your folder path\n",
    "output_csv_folder = 'D:/test/SAG_green_demo_board/'  # Folder to save results\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "aic_results = {}\n",
    "slopes_results = {}\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_csv_folder):\n",
    "    os.makedirs(output_csv_folder)\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # Ensure 'Intensity' and 'Count' columns have the same length\n",
    "        assert len(intensities) == len(counts), f\"Data length mismatch: {filename}\"\n",
    "        \n",
    "        # Reshape and prepare data\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # Find the best segment count using AIC\n",
    "        segment_range = range(2, 21)\n",
    "        aic_values = []\n",
    "        all_segment_slopes = []\n",
    "\n",
    "        for num_segments in segment_range:\n",
    "            aic, segment_slopes = calculate_aic(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "            all_segment_slopes.append(segment_slopes)\n",
    "\n",
    "        # Find the best number of segments based on AIC\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "\n",
    "        # Save the best segment count and corresponding slopes\n",
    "        aic_results[filename] = best_num_segments_aic\n",
    "        slopes_results[filename] = all_segment_slopes[best_num_segments_aic - 2]  # Using AIC-based best segment count\n",
    "\n",
    "# Prepare data for saving to AIC.csv\n",
    "aic_columns = []\n",
    "\n",
    "# Determine the maximum number of segments (which will define the number of slope columns)\n",
    "max_segments = 0\n",
    "for slopes in slopes_results.values():\n",
    "    max_segments = max(max_segments, len(slopes))\n",
    "\n",
    "# Prepare data for AIC CSV\n",
    "for filename in aic_results.keys():\n",
    "    # Get AIC and slopes for the best number of segments\n",
    "    best_aic = aic_results[filename]\n",
    "    slopes = slopes_results[filename]\n",
    "\n",
    "    # Pad the list with NaN if there are fewer slopes than the maximum number of segments\n",
    "    padded_slopes = slopes + [np.nan] * (max_segments - len(slopes))\n",
    "    aic_columns.append([filename, best_aic] + padded_slopes)\n",
    "\n",
    "# Generate column names for AIC CSV\n",
    "aic_column_names = ['Filename', 'Best_AIC']\n",
    "for i in range(1, max_segments + 1):  # Adjust to the max number of segments across all files\n",
    "    aic_column_names.append(f'Slope_{i}')\n",
    "\n",
    "# Create DataFrame for AIC\n",
    "aic_df = pd.DataFrame(aic_columns, columns=aic_column_names)\n",
    "\n",
    "# Save results to AIC.csv\n",
    "aic_df.to_csv(os.path.join(output_csv_folder, 'AIC.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記錄PLR端點斜率(AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define function to calculate AIC, slopes, and endpoints for each segment\n",
    "def calculate_aic_and_slopes(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # Sample size\n",
    "    k = 2  # Number of parameters per segment (slope and intercept)\n",
    "    residual_sum_of_squares = 0\n",
    "    segment_slopes = []  # To store the slopes of each segment\n",
    "    piecewise_endpoints = []  # To store the endpoints of each segment\n",
    "\n",
    "    # Perform linear regression on each segment and calculate residual sum of squares\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "\n",
    "        # Calculate the predicted values for the segment\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)\n",
    "\n",
    "        # Store the endpoints of the segment\n",
    "        piecewise_endpoints.append([(x_segment[0, 0], y_segment[0]), (x_segment[-1, 0], y_segment[-1])])\n",
    "\n",
    "    # Calculate AIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    \n",
    "    return aic, piecewise_endpoints\n",
    "\n",
    "# Function to calculate slopes from endpoints\n",
    "def calculate_slopes_from_endpoints(endpoints):\n",
    "    slopes = []\n",
    "    for start_end in endpoints:\n",
    "        # Extract start and end points\n",
    "        (x1, y1), (x2, y2) = start_end\n",
    "        if x2 - x1 != 0:  # To avoid division by zero\n",
    "            slope = (y2 - y1) / (x2 - x1)\n",
    "        else:\n",
    "            slope = np.nan  # If x2 == x1, slope is undefined (vertical line)\n",
    "        slopes.append(slope)\n",
    "    return slopes\n",
    "\n",
    "# Folder paths\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # Change to your folder path\n",
    "output_csv_folder = 'D:/test/SAG_green_demo_board/'  # Folder to save results\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "aic_results = {}\n",
    "slopes_results = {}\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(output_csv_folder):\n",
    "    os.makedirs(output_csv_folder)\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # Ensure 'Intensity' and 'Count' columns have the same length\n",
    "        assert len(intensities) == len(counts), f\"Data length mismatch: {filename}\"\n",
    "        \n",
    "        # Reshape and prepare data\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # Find the best segment count using AIC\n",
    "        segment_range = range(2, 21)\n",
    "        aic_values = []\n",
    "        all_endpoints = []\n",
    "\n",
    "        for num_segments in segment_range:\n",
    "            aic, endpoints = calculate_aic_and_slopes(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "            all_endpoints.append(endpoints)\n",
    "\n",
    "        # Find the best number of segments based on AIC\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "\n",
    "        # Save the best segment count, corresponding slopes, and endpoints\n",
    "        aic_results[filename] = best_num_segments_aic\n",
    "        best_endpoints = all_endpoints[best_num_segments_aic - 2]  # Using AIC-based best segment count\n",
    "        slopes_results[filename] = calculate_slopes_from_endpoints(best_endpoints)  # Calculate slopes from endpoints\n",
    "\n",
    "# Prepare data for saving to AIC.csv\n",
    "aic_columns = []\n",
    "\n",
    "# Determine the maximum number of segments (which will define the number of slope columns)\n",
    "max_segments = 0\n",
    "for slopes in slopes_results.values():\n",
    "    max_segments = max(max_segments, len(slopes))\n",
    "\n",
    "# Prepare data for AIC CSV\n",
    "for filename in aic_results.keys():\n",
    "    # Get AIC and slopes for the best number of segments\n",
    "    best_aic = aic_results[filename]\n",
    "    slopes = slopes_results[filename]\n",
    "\n",
    "    # Pad the list with NaN if there are fewer slopes than the maximum number of segments\n",
    "    padded_slopes = slopes + [np.nan] * (max_segments - len(slopes))\n",
    "    aic_columns.append([filename, best_aic] + padded_slopes)\n",
    "\n",
    "# Generate column names for AIC CSV\n",
    "aic_column_names = ['Filename', 'Best_AIC']\n",
    "for i in range(1, max_segments + 1):  # Adjust to the max number of segments across all files\n",
    "    aic_column_names.append(f'Slope_{i}')\n",
    "\n",
    "# Create DataFrame for AIC\n",
    "aic_df = pd.DataFrame(aic_columns, columns=aic_column_names)\n",
    "\n",
    "# Save results to AIC.csv\n",
    "aic_df.to_csv(os.path.join(output_csv_folder, 'AIC_PLR_endpoints_slope.csv'), index=False)\n",
    "\n",
    "print(\"AIC and slopes calculations completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLR_AIC(RA、RT、UT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义函数计算AIC\n",
    "def calculate_aic(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # 样本大小\n",
    "    k = 2  # 每个回归段的参数个数（线性回归有2个参数：斜率和截距）\n",
    "    \n",
    "    residual_sum_of_squares = 0\n",
    "\n",
    "    # 对每个区间进行线性回归并计算残差平方和\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 计算该段的预测值\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)  # 残差平方和\n",
    "\n",
    "    # 计算 AIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    \n",
    "    return aic\n",
    "\n",
    "# 读取数据并执行Piecewise回归的函数\n",
    "def piecewise_linear_regression(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    segments = []\n",
    "    piecewise_endpoints = []  # 用于保存每个分段的端点\n",
    "\n",
    "    # 对每个区间进行线性回归\n",
    "    model = LinearRegression()\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 记录每段的回归线\n",
    "        segments.append((x_segment, model.predict(x_segment)))\n",
    "        \n",
    "        # 保存该段的端点坐标（起始点和结束点）\n",
    "        piecewise_endpoints.append(((x_segment[0][0], y_segment[0]), (x_segment[-1][0], y_segment[-1])))\n",
    "\n",
    "    return breakpoints, segments, piecewise_endpoints\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv/'  # 更改为你的文件夹路径\n",
    "output_aic_folder = 'D:/test/SAG_green_demo_board/PLR_AIC_1416/'  # 保存结果的文件夹路径\n",
    "# folder_path = 'D:/test/SAG_green_demo_board/B_csv_RA/'  # 更改为你的文件夹路径\n",
    "# output_aic_folder = 'D:/test/SAG_green_demo_board/PLR_RA/'  # 保存结果的文件夹路径\n",
    "# folder_path = 'D:/test/SAG_green_demo_board/B_csv_RT/'  # 更改为你的文件夹路径\n",
    "# output_aic_folder = 'D:/test/SAG_green_demo_board/PLR_RT/'  # 保存结果的文件夹路径\n",
    "# folder_path = 'D:/test/SAG_green_demo_board/B_csv_UT/'  # 更改为你的文件夹路径\n",
    "# output_aic_folder = 'D:/test/SAG_green_demo_board/PLR_UT/'  # 保存结果的文件夹路径\n",
    "\n",
    "if not os.path.exists(output_aic_folder):\n",
    "    os.makedirs(output_aic_folder)\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保 'Intensity' 和 'Count' 列的长度一致\n",
    "        assert len(intensities) == len(counts), f\"数据列长度不一致: {filename}\"\n",
    "        \n",
    "        # 将数据转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 使用AIC选择最佳分段数\n",
    "        segment_range = range(14, 17)\n",
    "        aic_values = []\n",
    "        \n",
    "        for num_segments in segment_range:\n",
    "            aic = calculate_aic(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "\n",
    "        # 找到具有最小AIC的分段数\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "\n",
    "        # 基于AIC进行Piecewise回归并绘图\n",
    "        breakpoints_aic, segments_aic, piecewise_endpoints_aic = piecewise_linear_regression(x, y, best_num_segments_aic)\n",
    "\n",
    "        # 提取AIC的端点坐标\n",
    "        endpoints_x_aic = []\n",
    "        endpoints_y_aic = []\n",
    "        for start_end in piecewise_endpoints_aic:\n",
    "            endpoints_x_aic.extend([start_end[0][0], start_end[1][0]])  \n",
    "            endpoints_y_aic.extend([start_end[0][1], start_end[1][1]])   \n",
    "\n",
    "        # 绘制AIC的Piecewise回归结果\n",
    "        plt.plot(x, y, label='Original Data', color='b')  \n",
    "        plt.plot(endpoints_x_aic, endpoints_y_aic, label=f'Piecewise (AIC, {best_num_segments_aic} segments)', color='r', marker='o')  \n",
    "        plt.xlabel('Intensity')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Piecewise Linear Regression based on AIC ({best_num_segments_aic} Segments)')\n",
    "        plt.legend()\n",
    "\n",
    "        # 保存到AIC文件夹，文件名去掉 .csv 后缀\n",
    "        filename_without_extension = os.path.splitext(filename)[0]  # 去除 .csv 后缀\n",
    "        plt.savefig(os.path.join(output_aic_folder, f\"{filename_without_extension}.png\"))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 定义函数计算AIC\n",
    "def calculate_aic(x, y, num_segments):\n",
    "    breakpoints = np.linspace(0, len(x)-1, num_segments+1, dtype=int)\n",
    "    model = LinearRegression()\n",
    "    n = len(x)  # 样本大小\n",
    "    k = 2  # 每个回归段的参数个数（线性回归有2个参数：斜率和截距）  \n",
    "    residual_sum_of_squares = 0\n",
    "\n",
    "    # 对每个区间进行线性回归并计算残差平方和\n",
    "    for i in range(num_segments):\n",
    "        start, end = breakpoints[i], breakpoints[i+1]\n",
    "        x_segment = x[start:end+1]\n",
    "        y_segment = y[start:end+1]\n",
    "        model.fit(x_segment, y_segment)\n",
    "        \n",
    "        # 计算该段的预测值\n",
    "        y_pred = model.predict(x_segment)\n",
    "        residual_sum_of_squares += np.sum((y_segment - y_pred) ** 2)  # 残差平方和\n",
    "\n",
    "    # 计算 AIC\n",
    "    aic = n * np.log(residual_sum_of_squares / n) + 2 * k * num_segments\n",
    "    \n",
    "    return aic\n",
    "\n",
    "# 读取文件夹路径\n",
    "folder_path = 'D:/test/SAG_green_demo_board/B_csv_UT/'  # 更改为你的文件夹路径\n",
    "output_csv_folder = 'D:/test/SAG_green_demo_board/'  # 保存结果的文件夹路径\n",
    "\n",
    "# 初始化字典来存储AIC的最小分段数\n",
    "aic_results = {}\n",
    "\n",
    "if not os.path.exists(output_csv_folder):\n",
    "    os.makedirs(output_csv_folder)\n",
    "\n",
    "# 遍历文件夹中的所有CSV文件\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "        \n",
    "        intensities = df['Intensity'].values\n",
    "        counts = df['Count'].values\n",
    "        \n",
    "        # 确保 'Intensity' 和 'Count' 列的长度一致\n",
    "        assert len(intensities) == len(counts), f\"数据列长度不一致: {filename}\"\n",
    "        \n",
    "        # 将数据转换为合适的格式\n",
    "        x = intensities.reshape(-1, 1)\n",
    "        y = counts\n",
    "        \n",
    "        # 使用AIC选择最佳分段数\n",
    "        segment_range = range(2, 21)\n",
    "        aic_values = []\n",
    "        \n",
    "        for num_segments in segment_range:\n",
    "            aic = calculate_aic(x, y, num_segments)\n",
    "            aic_values.append(aic)\n",
    "\n",
    "        # 找到具有最小AIC的分段数\n",
    "        best_num_segments_aic = segment_range[np.argmin(aic_values)]\n",
    "\n",
    "        # 保存AIC的最小分段数\n",
    "        aic_results[filename] = best_num_segments_aic\n",
    "\n",
    "# 将AIC的最小分段数保存到CSV文件\n",
    "aic_df = pd.DataFrame(list(aic_results.items()), columns=['Filename', 'Best_AIC'])\n",
    "\n",
    "aic_df.to_csv(os.path.join(output_csv_folder, 'AIC_UT.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding(補0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_and_pad(image_path, target_size=(128, 128), padding_color=(0, 0, 0)):\n",
    "    # 打開圖像\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # 獲取當前圖像的尺寸\n",
    "    current_width, current_height = img.size\n",
    "    \n",
    "    # 計算縮放比例\n",
    "    ratio = min(target_size[0] / current_width, target_size[1] / current_height)\n",
    "    \n",
    "    # 計算新的尺寸\n",
    "    new_width = int(current_width * ratio)\n",
    "    new_height = int(current_height * ratio)\n",
    "    \n",
    "    # 縮放圖像\n",
    "    img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # 計算填充的尺寸\n",
    "    padding_left = (target_size[0] - new_width) // 2\n",
    "    padding_top = (target_size[1] - new_height) // 2\n",
    "    padding_right = target_size[0] - new_width - padding_left\n",
    "    padding_bottom = target_size[1] - new_height - padding_top\n",
    "    \n",
    "    # 在圖像周圍添加填充\n",
    "    img_padded = Image.new(\"RGB\", target_size, padding_color)\n",
    "    img_padded.paste(img_resized, (padding_left, padding_top))\n",
    "    \n",
    "    return img_padded\n",
    "\n",
    "# 設定資料夾路徑\n",
    "input_folder = 'D:/Training tool/RGB_DS_WL'  # 來源資料夾\n",
    "output_folder = 'D:/Training tool/RGB_DS_WL_PADZERO'  # 輸出資料夾\n",
    "\n",
    "# 如果輸出資料夾不存在，創建它\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍歷資料夾中的所有影像檔案\n",
    "for filename in os.listdir(input_folder):\n",
    "    # 檢查檔案是否為影像檔案\n",
    "    if filename.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg', '.tiff', '.gif')):\n",
    "        input_image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # 處理影像\n",
    "        output_image = resize_and_pad(input_image_path)\n",
    "        \n",
    "        # 提取原始檔案名稱和擴展名\n",
    "        base_name, ext = os.path.splitext(filename)\n",
    "        \n",
    "        # 構建新檔案名稱，加上 \"_padzero\" 後綴\n",
    "        new_image_path = os.path.join(output_folder, f\"{base_name}_padzero{ext}\")\n",
    "        \n",
    "        # 儲存處理後的影像\n",
    "        output_image.save(new_image_path)\n",
    "\n",
    "print(\"處理完成，所有影像已儲存到:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding(延伸邊界)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def extend_border_to_fill(image, target_size=(128, 128)):\n",
    "    # 获取当前图像的尺寸\n",
    "    current_width, current_height = image.size\n",
    "    \n",
    "    # 计算缩放比例，使图片最大化填满目标尺寸\n",
    "    ratio = min(target_size[0] / current_width, target_size[1] / current_height)\n",
    "    \n",
    "    # 计算新的尺寸\n",
    "    new_width = int(current_width * ratio)\n",
    "    new_height = int(current_height * ratio)\n",
    "    \n",
    "    # 缩放图像\n",
    "    img_resized = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # 计算填充的尺寸\n",
    "    padding_left = (target_size[0] - new_width) // 2\n",
    "    padding_top = (target_size[1] - new_height) // 2\n",
    "    padding_right = target_size[0] - new_width - padding_left\n",
    "    padding_bottom = target_size[1] - new_height - padding_top\n",
    "    \n",
    "    # 创建背景图片\n",
    "    img_padded = Image.new(\"RGB\", target_size)\n",
    "    img_padded.paste(img_resized, (padding_left, padding_top))\n",
    "\n",
    "    # 获取图像四个角的像素\n",
    "    top_left = image.getpixel((0, 0))\n",
    "    top_right = image.getpixel((current_width - 1, 0))\n",
    "    bottom_left = image.getpixel((0, current_height - 1))\n",
    "    bottom_right = image.getpixel((current_width - 1, current_height - 1))\n",
    "\n",
    "    # 扩展上边\n",
    "    for y in range(padding_top):\n",
    "        for x in range(target_size[0]):\n",
    "            if x < padding_left:\n",
    "                img_padded.putpixel((x, y), top_left)  # 扩展左上角颜色\n",
    "            elif x >= padding_left + new_width:\n",
    "                img_padded.putpixel((x, y), top_right)  # 扩展右上角颜色\n",
    "            else:\n",
    "                img_padded.putpixel((x, y), image.getpixel((x - padding_left, 0)))  # 上边颜色\n",
    "\n",
    "    # 扩展下边\n",
    "    for y in range(padding_top + new_height, target_size[1]):\n",
    "        for x in range(target_size[0]):\n",
    "            if x < padding_left:\n",
    "                img_padded.putpixel((x, y), bottom_left)  # 扩展左下角颜色\n",
    "            elif x >= padding_left + new_width:\n",
    "                img_padded.putpixel((x, y), bottom_right)  # 扩展右下角颜色\n",
    "            else:\n",
    "                img_padded.putpixel((x, y), image.getpixel((x - padding_left, current_height - 1)))  # 下边颜色\n",
    "\n",
    "    # 扩展左边\n",
    "    for x in range(padding_left):\n",
    "        for y in range(target_size[1]):\n",
    "            img_padded.putpixel((x, y), top_left)  # 扩展左边颜色\n",
    "\n",
    "    # 扩展右边\n",
    "    for x in range(padding_left + new_width, target_size[0]):\n",
    "        for y in range(target_size[1]):\n",
    "            img_padded.putpixel((x, y), top_right)  # 扩展右边颜色\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "# 设置文件夹路径\n",
    "input_folder = 'D:/Training tool/RGB_DS_WL'  # 输入文件夹路径\n",
    "output_folder = 'D:/Training tool/RGB_DS_WL_EXTENDBORDER'  # 输出文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历文件夹中的所有图像文件\n",
    "for filename in os.listdir(input_folder):\n",
    "    # 只处理图像文件\n",
    "    if filename.lower().endswith(('.bmp', '.jpg', '.jpeg', '.png', '.gif')):\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # 处理图像并保存\n",
    "        output_image = extend_border_to_fill(img)\n",
    "        \n",
    "        # 保存处理后的图像\n",
    "        output_filename = os.path.splitext(filename)[0] + '_extendborder' + os.path.splitext(filename)[1]\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        output_image.save(output_path)\n",
    "\n",
    "        print(f\"Processed and saved: {output_filename}\")\n",
    "\n",
    "print(\"All images have been processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算延伸邊界的異常影像數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_path = \"D:/Training tool/RGB_DS_WL_EXTENDBORDER\"\n",
    "\n",
    "# 定義顏色相似度檢查，容忍度設定為30（RGB每個分量的最大差異）\n",
    "def color_similar(c1, c2, tolerance=30):\n",
    "    return all(abs(c1[i] - c2[i]) <= tolerance for i in range(3))\n",
    "\n",
    "# 檢查影像邊界顏色是否一致（或相似）\n",
    "def check_border_color(image, tolerance=30):\n",
    "    width, height = image.size\n",
    "\n",
    "    # 確保影像是128x128\n",
    "    if width != 128 or height != 128:\n",
    "        return False\n",
    "    \n",
    "    # 讀取影像像素資料\n",
    "    pixels = image.load()\n",
    "\n",
    "    # 取得上邊界第一個像素作為參考顏色\n",
    "    top_left_color = pixels[0, 0]\n",
    "\n",
    "    # 檢查上邊界\n",
    "    for x in range(128):\n",
    "        if not color_similar(pixels[x, 0], top_left_color, tolerance):\n",
    "            return False\n",
    "    \n",
    "    # 檢查下邊界\n",
    "    for x in range(128):\n",
    "        if not color_similar(pixels[x, 127], top_left_color, tolerance):\n",
    "            return False\n",
    "    \n",
    "    # 檢查左邊界\n",
    "    for y in range(128):\n",
    "        if not color_similar(pixels[0, y], top_left_color, tolerance):\n",
    "            return False\n",
    "    \n",
    "    # 檢查右邊界\n",
    "    for y in range(128):\n",
    "        if not color_similar(pixels[127, y], top_left_color, tolerance):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# 用來存放符合條件的影像檔案\n",
    "valid_images = []\n",
    "\n",
    "# 遍歷資料夾中的所有檔案\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 檢查是否為影像檔案（可以根據檔案副檔名過濾）\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "        # 生成檔案的完整路徑\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # 讀取影像\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # 檢查影像是否符合條件（128x128 且邊界顏色一致）\n",
    "        if check_border_color(img, tolerance=30):\n",
    "            valid_images.append(filename)\n",
    "\n",
    "# 顯示符合條件的影像檔案\n",
    "print(f\"符合條件的影像有 {len(valid_images)} 張。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 灰階轉為RGB顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 開啟影像檔案\n",
    "image_path = 'D:/AI_inference/Central_Science/DataVisualization/50459++PL2432_1~1_RGB_padzero_label.png'  # 請將此路徑替換為你的影像檔案路徑\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 轉換為灰階\n",
    "gray_image = image.convert('L')\n",
    "\n",
    "# 取得影像的像素資料\n",
    "pixels = gray_image.load()\n",
    "\n",
    "# 取得影像的尺寸\n",
    "width, height = gray_image.size\n",
    "\n",
    "# 創建新的 RGB 圖像\n",
    "new_image = Image.new(\"RGB\", (width, height))\n",
    "new_pixels = new_image.load()\n",
    "\n",
    "# 根據灰階值映射到 RGB\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        gray_value = pixels[x, y]\n",
    "        \n",
    "        # 根據灰階值映射到顏色\n",
    "        if gray_value == 0:\n",
    "            new_pixels[x, y] = (255, 0, 0)  # R\n",
    "        elif gray_value == 1:\n",
    "            new_pixels[x, y] = (0, 255, 0)  # G\n",
    "        elif gray_value == 2:\n",
    "            new_pixels[x, y] = (0, 0, 255)  # B\n",
    "        elif gray_value == 255:\n",
    "            new_pixels[x, y] = (255, 255, 255)  # White\n",
    "\n",
    "# 顯示新的影像\n",
    "new_image.show()\n",
    "\n",
    "# 儲存新的影像\n",
    "new_image.save('D:/AI_inference/Central_Science/DataVisualization/output_image.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 灰階轉為RGB顯示 & 計算pixel數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# 開啟影像檔案\n",
    "image_path = 'D:/AI_inference/Central_Science/DataVisualization/316++11419++PL2432_1~1_RGB_predict.png'  # 請將此路徑替換為你的影像檔案路徑\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# 轉換為灰階\n",
    "gray_image = image.convert('L')\n",
    "\n",
    "# 取得影像的像素資料\n",
    "pixels = gray_image.load()\n",
    "\n",
    "# 取得影像的尺寸\n",
    "width, height = gray_image.size\n",
    "\n",
    "# 創建新的 RGB 圖像\n",
    "new_image = Image.new(\"RGB\", (width, height))\n",
    "new_pixels = new_image.load()\n",
    "\n",
    "# 初始化計數變數\n",
    "red_count = 0\n",
    "green_count = 0\n",
    "blue_count = 0\n",
    "white_count = 0\n",
    "\n",
    "# 根據灰階值映射到 RGB 並計算顏色數量\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        gray_value = pixels[x, y]\n",
    "        \n",
    "        # 根據灰階值映射到顏色\n",
    "        if gray_value == 0:\n",
    "            new_pixels[x, y] = (255, 0, 0)  # R\n",
    "            red_count += 1  # 累加紅色像素數量\n",
    "        elif gray_value == 1:\n",
    "            new_pixels[x, y] = (0, 255, 0)  # G\n",
    "            green_count += 1  # 累加綠色像素數量\n",
    "        elif gray_value == 2:\n",
    "            new_pixels[x, y] = (0, 0, 255)  # B\n",
    "            blue_count += 1  # 累加藍色像素數量\n",
    "        elif gray_value == 255:\n",
    "            new_pixels[x, y] = (255, 255, 255)  # White\n",
    "            white_count += 1  # 累加白色像素數量\n",
    "\n",
    "# 顯示新的影像\n",
    "new_image.show()\n",
    "\n",
    "# 儲存新的影像\n",
    "new_image.save('D:/AI_inference/Central_Science/DataVisualization/output_image.jpg')\n",
    "\n",
    "# 輸出顏色數量\n",
    "print(f\"紅色像素數量: {red_count}\")\n",
    "print(f\"綠色像素數量: {green_count}\")\n",
    "print(f\"藍色像素數量: {blue_count}\")\n",
    "print(f\"白色像素數量: {white_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四種內插方法(NN/BiLinear/BiCubic/Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_and_pad(image_path, target_size=(128, 128), padding_color=(0, 0, 0), resample_method=Image.Resampling.NEAREST):\n",
    "    # 打開圖像\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # 獲取當前圖像的尺寸\n",
    "    current_width, current_height = img.size\n",
    "    \n",
    "    # 計算縮放比例\n",
    "    ratio = min(target_size[0] / current_width, target_size[1] / current_height)\n",
    "    \n",
    "    # 計算新的尺寸\n",
    "    new_width = int(current_width * ratio)\n",
    "    new_height = int(current_height * ratio)\n",
    "    \n",
    "    # 使用選定的內插方法縮放圖像\n",
    "    img_resized = img.resize((new_width, new_height), resample_method)\n",
    "    \n",
    "    # 計算填充的尺寸\n",
    "    padding_left = (target_size[0] - new_width) // 2\n",
    "    padding_top = (target_size[1] - new_height) // 2\n",
    "    padding_right = target_size[0] - new_width - padding_left\n",
    "    padding_bottom = target_size[1] - new_height - padding_top\n",
    "    \n",
    "    # 在圖像周圍添加填充\n",
    "    img_padded = Image.new(\"RGB\", target_size, padding_color)\n",
    "    img_padded.paste(img_resized, (padding_left, padding_top))\n",
    "    \n",
    "    return img_padded\n",
    "\n",
    "# 設定資料夾路徑\n",
    "input_folder = 'D:/Training tool/RGB_DS_WL'  # 來源資料夾\n",
    "output_folder = 'D:/Training tool'  # 輸出資料夾\n",
    "\n",
    "# 如果輸出資料夾不存在，創建它\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 內插方法選擇：NN, Linear, Cubic, LANCZOS\n",
    "resample_methods = {\n",
    "    'NN': Image.Resampling.NEAREST,         # Nearest Neighbor\n",
    "    'Linear': Image.Resampling.BILINEAR,    # Bilinear\n",
    "    'Cubic': Image.Resampling.BICUBIC,      # Bicubic\n",
    "    'LANCZOS': Image.Resampling.LANCZOS     # Lanczos\n",
    "}\n",
    "\n",
    "# 為每個內插方法創建對應的資料夾\n",
    "for method_name in resample_methods.keys():\n",
    "    method_folder = os.path.join(output_folder, method_name)\n",
    "    if not os.path.exists(method_folder):\n",
    "        os.makedirs(method_folder)\n",
    "\n",
    "# 遍歷資料夾中的所有影像檔案\n",
    "for filename in os.listdir(input_folder):\n",
    "    # 檢查檔案是否為影像檔案\n",
    "    if filename.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg', '.tiff', '.gif')):\n",
    "        input_image_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # 依次使用不同的內插方法處理影像\n",
    "        for method_name, resample_method in resample_methods.items():\n",
    "            # 處理影像\n",
    "            output_image = resize_and_pad(input_image_path, resample_method=resample_method)\n",
    "            \n",
    "            # 提取原始檔案名稱和擴展名\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            \n",
    "            # 構建新檔案名稱，加上內插方法名稱後綴\n",
    "            new_image_path = os.path.join(output_folder, method_name, f\"{base_name}_{method_name}{ext}\")\n",
    "            \n",
    "            # 儲存處理後的影像\n",
    "            output_image.save(new_image_path)\n",
    "\n",
    "print(\"處理完成，所有影像已儲存到:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算內插法運算時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "def resize_and_pad(image_path, target_size=(128, 128), padding_color=(0, 0, 0), resample_method=Image.Resampling.NEAREST):\n",
    "    \"\"\"進行影像縮放和填充\"\"\"\n",
    "    # 打開圖像\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # 獲取當前圖像的尺寸\n",
    "    current_width, current_height = img.size\n",
    "    \n",
    "    # 計算縮放比例\n",
    "    ratio = min(target_size[0] / current_width, target_size[1] / current_height)\n",
    "    \n",
    "    # 計算新的尺寸\n",
    "    new_width = int(current_width * ratio)\n",
    "    new_height = int(current_height * ratio)\n",
    "    \n",
    "    # 使用選定的內插方法縮放圖像\n",
    "    img_resized = img.resize((new_width, new_height), resample_method)\n",
    "    \n",
    "    # 計算填充的尺寸\n",
    "    padding_left = (target_size[0] - new_width) // 2\n",
    "    padding_top = (target_size[1] - new_height) // 2\n",
    "    padding_right = target_size[0] - new_width - padding_left\n",
    "    padding_bottom = target_size[1] - new_height - padding_top\n",
    "    \n",
    "    # 在圖像周圍添加填充\n",
    "    img_padded = Image.new(\"RGB\", target_size, padding_color)\n",
    "    img_padded.paste(img_resized, (padding_left, padding_top))\n",
    "    \n",
    "    return img_padded\n",
    "\n",
    "def process_images(input_folder, output_folder, resample_method, method_name):\n",
    "    \"\"\"處理影像並儲存，計算執行時間\"\"\"\n",
    "    # 為每個內插方法創建對應的資料夾\n",
    "    method_folder = os.path.join(output_folder, method_name)\n",
    "    if not os.path.exists(method_folder):\n",
    "        os.makedirs(method_folder)\n",
    "\n",
    "    start_time = time.time()  # 計時開始\n",
    "\n",
    "    # 遍歷資料夾中的所有影像檔案\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # 檢查檔案是否為影像檔案\n",
    "        if filename.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg', '.tiff', '.gif')):\n",
    "            input_image_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # 處理影像\n",
    "            output_image = resize_and_pad(input_image_path, resample_method=resample_method)\n",
    "            \n",
    "            # 提取原始檔案名稱和擴展名\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            \n",
    "            # 構建新檔案名稱，加上內插方法名稱後綴\n",
    "            new_image_path = os.path.join(method_folder, f\"{base_name}_{method_name}{ext}\")\n",
    "            \n",
    "            # 儲存處理後的影像\n",
    "            output_image.save(new_image_path)\n",
    "\n",
    "    end_time = time.time()  # 計時結束\n",
    "    elapsed_time = end_time - start_time  # 計算時間差\n",
    "    print(f\"內插方法 {method_name} 處理完成，耗時: {elapsed_time:.2f} 秒\")\n",
    "\n",
    "def main():\n",
    "    input_folder = 'D:/Training tool/RGB_DS_WL'  # 來源資料夾\n",
    "    output_folder = 'D:/Training tool/00000000000000000000000000000000'  # 輸出資料夾\n",
    "\n",
    "    # 內插方法選擇：NN, Linear, Cubic, LANCZOS\n",
    "    resample_methods = {\n",
    "        'NN': Image.Resampling.NEAREST,         # Nearest Neighbor\n",
    "        'Linear': Image.Resampling.BILINEAR,    # Bilinear\n",
    "        'Cubic': Image.Resampling.BICUBIC,      # Bicubic\n",
    "        'LANCZOS': Image.Resampling.LANCZOS     # Lanczos\n",
    "    }\n",
    "\n",
    "    # 將每種內插方法的處理分開執行並計算時間\n",
    "    for method_name, resample_method in resample_methods.items():\n",
    "        process_images(input_folder, output_folder, resample_method, method_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算SSIM/PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import csv\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"計算兩幅圖像的 PSNR\"\"\"\n",
    "    return cv2.PSNR(img1, img2)\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    \"\"\"計算兩幅圖像的 SSIM\"\"\"\n",
    "    # 轉換為灰度圖像\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    return ssim(img1_gray, img2_gray)\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"加載圖像並轉換為 numpy 陣列\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"錯誤：無法加載圖像 {image_path}.\")\n",
    "    return img\n",
    "\n",
    "# 設定資料夾路徑\n",
    "input_folder = 'D:/Training tool/00000000000000000000000000000000/RGB_DS_WL_PADZERO'  # 來源資料夾（原圖）\n",
    "output_folder = 'D:/Training tool/00000000000000000000000000000000/LANCZOS'  # 輸出資料夾（處理後圖像）\n",
    "\n",
    "# 確保輸出資料夾存在\n",
    "if not os.path.exists(output_folder):\n",
    "    print(f\"錯誤：輸出資料夾 {output_folder} 不存在。\")\n",
    "    exit(1)\n",
    "\n",
    "# 準備 CSV 檔案\n",
    "csv_file_path = 'D:/Training tool/00000000000000000000000000000000/image_quality_results_LANCZOS.csv'\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # 寫入 CSV 標題\n",
    "    writer.writerow(['Image Filename', 'PSNR', 'SSIM'])\n",
    "\n",
    "    # 遍歷原圖資料夾中的所有影像檔案\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # 檢查檔案是否為影像檔案\n",
    "        if filename.lower().endswith(('.bmp', '.jpg', '.png', '.jpeg', '.tiff', '.gif')):\n",
    "            # 根據共同部分 J101_1~10_RGB 提取檔名\n",
    "            base_name = \"_\".join(filename.split('_')[:3])\n",
    "            \n",
    "            input_image_path = os.path.join(input_folder, filename)\n",
    "            output_image_path = os.path.join(output_folder, f\"{base_name}_LANCZOS.bmp\")  # 假設處理後的圖像為 _NN\n",
    "\n",
    "            # 檢查處理過的圖像是否存在\n",
    "            if not os.path.exists(output_image_path):\n",
    "                print(f\"錯誤：處理後的圖像 {output_image_path} 不存在。跳過此文件。\")\n",
    "                continue\n",
    "            \n",
    "            # 加載原始影像和處理後的影像\n",
    "            original_image = load_image(input_image_path)\n",
    "            output_image = load_image(output_image_path)\n",
    "            \n",
    "            # 確保圖像已經成功加載\n",
    "            if original_image is None or output_image is None:\n",
    "                continue  # 跳過無法加載的影像\n",
    "            \n",
    "            # 計算 PSNR 和 SSIM\n",
    "            psnr_value = calculate_psnr(original_image, output_image)\n",
    "            ssim_value = calculate_ssim(original_image, output_image)\n",
    "            \n",
    "            # 將結果寫入 CSV 檔案\n",
    "            writer.writerow([base_name, f\"{psnr_value:.2f}\", f\"{ssim_value:.4f}\"])\n",
    "\n",
    "print(f\"所有影像的 PSNR 和 SSIM 計算完成，結果已儲存到 {csv_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改檔案名稱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 設定資料夾路徑\n",
    "folder_A = 'D:/AI_inference/Central_Science/Data/Label_interpolation_backup'\n",
    "folder_B = 'D:/AI_inference/Central_Science/Data/Image_interpolation_backup'\n",
    "\n",
    "# 取得資料夾裡的檔案名稱列表\n",
    "files_A = sorted(os.listdir(folder_A))\n",
    "files_B = sorted(os.listdir(folder_B))\n",
    "\n",
    "# 確保兩個資料夾裡檔案數量相同\n",
    "if len(files_A) == len(files_B):\n",
    "    # 第一部分：將資料夾 A 的檔案名稱更改為資料夾 B 的檔案名稱\n",
    "    for file_A, file_B in zip(files_A, files_B):\n",
    "        # 取得檔案的完整路徑\n",
    "        file_A_path = os.path.join(folder_A, file_A)\n",
    "        file_B_path = os.path.join(folder_B, file_B)\n",
    "        \n",
    "        # 確保是檔案而不是資料夾\n",
    "        if os.path.isfile(file_A_path):\n",
    "            # 取得檔案 A 的副檔名\n",
    "            file_A_name, file_A_ext = os.path.splitext(file_A)\n",
    "            \n",
    "            # 取得檔案 B 的名稱，不包含副檔名\n",
    "            file_B_name, file_B_ext = os.path.splitext(file_B)\n",
    "            \n",
    "            # 確保不會更改副檔名\n",
    "            if file_A_ext == '.png':  # 假設 A 資料夾的檔案是 .png\n",
    "                new_name = file_B_name + file_A_ext  # 保留 A 的副檔名 (.png)\n",
    "            else:\n",
    "                new_name = file_B_name + file_A_ext\n",
    "\n",
    "            # 生成新檔案的完整路徑\n",
    "            new_file_A_path = os.path.join(folder_A, new_name)\n",
    "\n",
    "            # 更改檔案名稱\n",
    "            try:\n",
    "                os.rename(file_A_path, new_file_A_path)\n",
    "                print(f\"檔案已更名: {file_A} -> {new_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"更名失敗: {file_A} -> {new_name}, 錯誤訊息: {e}\")\n",
    "else:\n",
    "    print(\"A和B資料夾中的檔案數量不同！\")\n",
    "\n",
    "# 第二部分：在資料夾 A 中檔案名稱末尾新增 \"_label\"\n",
    "for filename in os.listdir(folder_A):\n",
    "    # 檢查檔案是否為檔案而非資料夾\n",
    "    if os.path.isfile(os.path.join(folder_A, filename)):\n",
    "        # 輸出檔案名稱，檢查是否有匹配的檔案\n",
    "        print(f\"檢查檔案: {filename}\")\n",
    "\n",
    "        # 分割檔案名和副檔名\n",
    "        name, ext = os.path.splitext(filename)\n",
    "\n",
    "        # 創建新的檔名，新增 \"_label\" 在檔名的末尾，但不改變副檔名\n",
    "        new_filename = f\"{name}_label{ext}\"\n",
    "\n",
    "        # 獲取完整檔案路徑\n",
    "        old_file = os.path.join(folder_A, filename)\n",
    "        new_file = os.path.join(folder_A, new_filename)\n",
    "\n",
    "        # 重新命名檔案\n",
    "        try:\n",
    "            os.rename(old_file, new_file)\n",
    "            print(f\"檔案已更名: {filename} -> {new_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"更名失敗: {filename} -> {new_filename}, 錯誤訊息: {e}\")\n",
    "\n",
    "print(\"檔案名稱修改完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去除Label Tool的排序(++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 目標資料夾路徑\n",
    "folder_path = 'D:/AI_inference/Central_Science/Data/Label'\n",
    "\n",
    "# 獲取資料夾中的所有檔案\n",
    "for filename in os.listdir(folder_path):\n",
    "    # 檢查檔案是否為檔案而非資料夾\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        # 輸出檔案名稱，檢查是否有匹配的檔案\n",
    "        print(f\"檢查檔案: {filename}\")\n",
    "        \n",
    "        # 尋找 '++' 位置，並保留其後的部分\n",
    "        if '++' in filename:\n",
    "            new_filename = filename.split('++', 1)[-1]  # 只取 '++' 之後的部分\n",
    "        else:\n",
    "            new_filename = filename  # 若檔案名稱中沒有 '++'，則不更改\n",
    "        \n",
    "        # 獲取完整檔案路徑\n",
    "        old_file = os.path.join(folder_path, filename)\n",
    "        new_file = os.path.join(folder_path, new_filename)\n",
    "        \n",
    "        # 重新命名檔案\n",
    "        try:\n",
    "            os.rename(old_file, new_file)\n",
    "            print(f\"檔案已更名: {filename} -> {new_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"更名失敗: {filename} -> {new_filename}, 錯誤訊息: {e}\")\n",
    "\n",
    "print(\"檔案名稱修改完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
